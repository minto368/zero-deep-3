{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第５ステージ　DeZeroで挑む"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ52　GPU対応\n",
    "\n",
    "### 52.1 - 52.4\n",
    "省略\n",
    "\n",
    "### 52.5　GPUでMNISTを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import leopard\n",
    "import leopard.functions as F\n",
    "from leopard import optimizers\n",
    "from leopard import DataLoader\n",
    "from leopard.models import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 5\n",
    "batch_size = 100\n",
    "\n",
    "train_set = leopard.datasets.MNIST(train=True)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "model = MLP((1000, 10))\n",
    "optimizer = optimizers.SGD().setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU mode\n",
    "if leopard.cuda.gpu_enable:\n",
    "    train_loader.to_gpu()\n",
    "    model.to_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.9084, time: 2.1086[sec]\n",
      "epoch: 2, loss: 1.2747, time: 1.8774[sec]\n",
      "epoch: 3, loss: 0.9189, time: 1.8558[sec]\n",
      "epoch: 4, loss: 0.7362, time: 1.8578[sec]\n",
      "epoch: 5, loss: 0.6323, time: 1.8633[sec]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    start = time.time()\n",
    "    sum_loss = 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    print('epoch: {}, loss: {:.4f}, time: {:.4f}[sec]'.format(\n",
    "        epoch + 1, sum_loss / len(train_set), elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試しにCPUモードで学習してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size)\n",
    "model = MLP((1000, 10))\n",
    "optimizer = optimizers.SGD().setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.to_cpu()\n",
    "model.to_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.9127, time: 4.4109[sec]\n",
      "epoch: 2, loss: 1.2858, time: 4.4089[sec]\n",
      "epoch: 3, loss: 0.9272, time: 4.3324[sec]\n",
      "epoch: 4, loss: 0.7422, time: 4.0596[sec]\n",
      "epoch: 5, loss: 0.6371, time: 4.0639[sec]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    start = time.time()\n",
    "    sum_loss = 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    print('epoch: {}, loss: {:.4f}, time: {:.4f}[sec]'.format(\n",
    "        epoch + 1, sum_loss / len(train_set), elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ53 モデルの保存と読み込み\n",
    "\n",
    "### 53.1 NumPyのsave関数とload関数\n",
    "省略\n",
    "\n",
    "### 53.2 Layerクラスのパラメータをフラットに"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from leopard import Layer\n",
    "from leopard import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Layer()\n",
    "\n",
    "l1 = Layer()\n",
    "l1.p1 = Parameter(np.array(1))\n",
    "\n",
    "layer.l1 = l1\n",
    "layer.p2 = Parameter(np.array(2))\n",
    "layer.p3 = Parameter(np.array(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l1/p1': variable(1), 'p3': variable(3), 'p2': variable(2)}\n"
     ]
    }
   ],
   "source": [
    "params_dict = {}\n",
    "layer._flatten_params(params_dict)\n",
    "print(params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 53.3 Layerクラスのsave関数とload関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 3\n",
    "batch_size = 100\n",
    "\n",
    "train_set = leopard.datasets.MNIST(train=True)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "model = MLP((1000, 10))\n",
    "optimizer = optimizers.SGD().setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータの読み込み\n",
    "if os.path.exists('my_mlp.npz'):\n",
    "    model.load_weights('my_mlp.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.5194\n",
      "epoch: 2, loss: 0.4867\n",
      "epoch: 3, loss: 0.4620\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    sum_loss = 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "\n",
    "    print('epoch: {}, loss: {:.4f}'.format(\n",
    "        epoch + 1, sum_loss / len(train_set)))\n",
    "\n",
    "# パラメータの保存\n",
    "model.save_weights('my_mlp.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ54　Dropoutとテストモード\n",
    "例えば、10個のニューロンからなる層があり、その層の次にDropoutレイヤを用いて60%のニューロンをランダムで消去する。つまり、毎回平均して４つの出力だけが次の層へ伝達し、残りは０になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False  True  True  True  True  True False False]\n",
      "[1. 0. 0. 1. 1. 1. 1. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "dropout_ratio  = 0.6\n",
    "x = np.ones(10)\n",
    "\n",
    "mask = np.random.rand(10) > dropout_ratio\n",
    "print(mask)\n",
    "\n",
    "y = x * mask\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論時には、全てのニューロンを使いながら、学習時の挙動を**真似る**必要がある。それには、全てのニューロンを使って計算し、その出力を**弱める**ことで対応できる。<br>\n",
    "\n",
    "先ほどの例では、学習時平均して40%のニューロンが使用される。それを考慮して、推論時には、全てのニューロンを使って計算し、出力を0.4倍する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習時\n",
    "mask = np.random.rand(*x.shape) > dropout_ratio\n",
    "y = x * mask\n",
    "\n",
    "# 推論時\n",
    "scale = 1 - dropout_ratio\n",
    "y = x * scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 54.2　Inverted Dropout\n",
    "先ほど、**推論時**に`scale`を乗算した。ここで、推論時に何も行わないように、学習時に先回りしてニューロンの値を`1 / scale`倍する。そうすることで主に以下の２つのような利点がある。\n",
    "\n",
    "1. 推論時の処理速度が少し向上する\n",
    "2. dropout_ratioを動的に変更できる  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習時\n",
    "scale = 1 - dropout_ratio\n",
    "mask = np.random.rand(*x.shape) > dropout_ratio\n",
    "y = x * mask / scale\n",
    "\n",
    "# 推論時\n",
    "y = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 54.3 テストモードの追加\n",
    "Dropoutを使うには、学習と推論のフェーズを判別する必要がある。そこで、逆伝播が不要なモード(`with leopard.no_grad():`)の仕組みを流用する。\n",
    "\n",
    "### 54.4　Dropoutの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "variable([2. 0. 2. 0. 0.])\n",
      "variable([1. 1. 1. 1. 1.])\n"
     ]
    }
   ],
   "source": [
    "from leopard import test_mode\n",
    "\n",
    "x = np.ones(5)\n",
    "print(x)\n",
    "\n",
    "# 学習時\n",
    "y = F.dropout(x)\n",
    "print(y)\n",
    "\n",
    "# 推論時\n",
    "with test_mode():\n",
    "    y = F.dropout(x)\n",
    "    print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d62b7059efc6466fc95c4734c71a3d4ec880ec93371e3e4a23230bdabcccc75d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
